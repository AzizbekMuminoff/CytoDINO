# ðŸ”¬ CytoDINO: Bone Marrow Cell Classification model based on DinoV3

A computationally efficient approach to bone marrow cell classification achieving **state-of-the-art results** on the MLL dataset by fine-tuning DINOv3 with LoRA adapters.

## Highlights

- **SOTA Performance**: 87.7% Weighted F1, 75.4% Macro F1 â€” outperforming DAGDNet* and DinoBloom
- **Efficient Fine-tuning**: LoRA injection enables training on consumer GPUs with only ~5-8% trainable parameters
- **Biologically-Informed Loss**: Custom hierarchical label smoothing respecting cell lineage relationships
- **Clinical Safety**: Critical error penalty to minimize dangerous misclassifications (e.g., blast to normal)

## Summary Metrics

| Metric | DenseNet | DenseNet* | DAGDNet | DAGDNet* | DinoBloom-L | DinoBloom-G | CytoDINO-32 (Proposed) | CytoDINO-64 (Proposed) |
|--------|:--------:|:---------:|:-------:|:--------:|:-----------:|:-----------:|:------------------:|:------------------:|
| Weighted Precision | 84.9% | 87.4% | 85.6% | 88.1% | â€” | â€” | 88.0% | **88.1%** |
| Weighted F1 | 84.5% | 87.1% | 84.9% | 87.8% | 84.9% | 84.9% | **87.7%** | 87.5% |
| Macro F1 | 55.8% | 64.0% | 55.3% | 71.5% | â€” | â€” | **75.4%** | 75.3% |
| Balanced Accuracy | â€” | â€” | â€” | â€” | 64.4% | 69.3% | **76.7%**â€  | 76.3%â€  |
| Accuracy | â€” | â€” | â€” | â€” | 85.0% | 85.0% | **87.5%** | 87.2% |

> **Note:** Our models achieve state-of-the-art Macro F1 scores (~75%), representing a **+3.9%** improvement over DAGDNet* (71.5%), indicating superior performance on minority classes.

# Quick Start

```bash
# Clone repository
git clone https://github.com/azizbekmuminoff/CytoDINO
cd CytoDINO

# Install dependencies
pip install -r requirements.txt

# Train model
python train.py
```

The dataset is automatically downloaded from Kaggle via `kagglehub`.

## Method

### Architecture
We fine-tune **DINOv3-ViT-L** (~300mln parameters) using **LoRA**, keeping the backbone frozen while injecting trainable adapters into all linear layers. A lightweight **Transformer Decoder Head** aggregates patch features for classification.

### Key Innovations

**1. Hierarchical Label Smoothing**

Instead of uniform label smoothing, we distribute smoothing mass based on biological cell lineages:

```
Granulocytic: BLA â†’ PMO â†’ MYB â†’ MMZ â†’ NGB â†’ NGS
Erythroid:    PEB â†’ EBO
Lymphoid:     LYI â†’ LYT â†’ PLM
```

Misclassifications within the same lineage receive higher smoothing weight, with adjacent maturation stages receiving additional bonus. This guides the model to make "biologically reasonable" errors.

**2. Critical Error Penalty**

We apply a 3Ã— loss multiplier when the model misclassifies critical cells (blasts, promyelocytes, faggot cells, etc.) as benign cells. This reduces clinically dangerous false negatives.

**3. Balanced Oversampling**

Class-weighted sampling with inverse frequency weighting (power 0.7) addresses the severe class imbalance (e.g., 8 abnormal eosinophils vs 5,884 segmented neutrophils).
